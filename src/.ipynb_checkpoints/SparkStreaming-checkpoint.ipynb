{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, NaiveBayesModel, LogisticRegressionModel\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer,RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import concat, col, lit, monotonically_increasing_id\n",
    "from pyspark.ml.linalg import Vectors, Vector\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "import nltk\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import lit\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_twitter_stream(model, sc):\n",
    "\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "    bootstrapServers = \"localhost:9092\"\n",
    "    topics = \"<your_kafka_topic_name>\"\n",
    "\n",
    "    \n",
    "    socketDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrapServers)\\\n",
    "    .option(\"subscribe\", topics)\\\n",
    "    .load()\\\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "    socketDF.printSchema()\n",
    "    print(\"COLUMNS:\", socketDF.columns)\n",
    "\n",
    "    socketDF = socketDF.withColumn(\"label\", lit(0))\n",
    "    socketDF = socketDF.withColumnRenamed(\"value\", \"status\")\n",
    "\n",
    "    prediction = getTweetSentiment(model, socketDF)\n",
    "\n",
    "    prediction.printSchema()\n",
    "        \n",
    "    print(prediction.schema)\n",
    "    \n",
    "    #count positive and negative tweets\n",
    "    prediction = prediction.groupBy(\"prediction\") \\\n",
    "                    .count().alias('sentiment_sum')\n",
    "\n",
    "    #results on console\n",
    "    query1 = prediction.writeStream.outputMode(\"update\").format('console').start()\n",
    "\n",
    "    query1.awaitTermination()\n",
    "    \n",
    "\n",
    "def loadDataset(sc, path):\n",
    "    \n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "    cols = [\"text\",\"label\"]\n",
    "\n",
    "    schema = [StructField('text', StringType(), True),StructField('label', DoubleType(), True)]\n",
    "    schema = schema = StructType(schema)\n",
    "\n",
    "    df = spark.read.csv(path, schema=schema)\n",
    "\n",
    "    return (df, spark)\n",
    "\n",
    "\n",
    "def createNBModel(sc):\n",
    "\n",
    "    (df, spark) = loadDataset(sc, \"grtweetdataset/grTweets.csv\")\n",
    "\n",
    "    hashingTF = HashingTF(inputCol=\"text\", outputCol='featTF')\n",
    "    idf = IDF(inputCol = hashingTF.getOutputCol() , outputCol ='features')\n",
    "\n",
    "    nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashingTF, nb])\n",
    "\n",
    "    # Train the data\n",
    "    model = pipeline.fit(df)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def getTweetSentiment(model, df):\n",
    "    prediction = model.transform(df)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# might need to add some jars for structured streaming- kafka\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1 pyspark-shell'\n",
    "\n",
    "# Initialize the spark config\n",
    "conf = SparkConf().setAppName('TwitterStream').setMaster(\"local[*]\")\n",
    "\n",
    "# Create the spark context\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# Suppress debug messages\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create Naive Bayes model\n",
    "model = createNBModel(sc)\n",
    "\n",
    "process_twitter_stream(model, sc)\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
