{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pika\n",
    "import json\n",
    "import redis\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tweep\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_HOST = 'localhost'\n",
    "REDIS_PORT = 6379\n",
    "REDIS_LIST = '<your_relis_list_name>'\n",
    "counter = 0\n",
    "\n",
    "# Try to connect to the RabbitMQ server\n",
    "try:\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n",
    "    channel = connection.channel()\n",
    "\n",
    "    # Create a queue called <your_queue_name>\n",
    "    channel.queue_declare(queue='<your_queue_name>')\n",
    "\n",
    "except Exception as err:\n",
    "    print (err)\n",
    "\n",
    "# try to connect to the redis server\n",
    "try:\n",
    "    r = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n",
    "except Exception as err:\n",
    "    print (err)\n",
    "\n",
    "\n",
    "def callback(ch, method, properties, body):\n",
    "    global counter\n",
    "    print(body.decode())\n",
    "\n",
    "    try:\n",
    "        r.lpush(REDIS_LIST, body.decode())\n",
    "        counter += 1\n",
    "    except:\n",
    "        print ('Problem adding data to Redis.')\n",
    "\n",
    "    # stop after 1000 tweets (optional - can be removed)\n",
    "    if counter == 1000:\n",
    "        channel.close()\n",
    "        connection.close()\n",
    "\n",
    "\n",
    "channel.basic_consume('<your_queue_name>', callback, True)\n",
    "\n",
    "print(' [*] Waiting for messages. To exit press CTRL+C')\n",
    "\n",
    "channel.start_consuming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take redis entries\n",
    "result = r.lrange(REDIS_LIST, 0, -1)\n",
    "p = re.compile(r'@([^\\s:]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode and remove unicode characters\n",
    "newList = []\n",
    "for i in result:\n",
    "    i = i.decode(\"utf-8\")\n",
    "    if \"\\\\\" not in i:\n",
    "        newList.append(i)\n",
    "        \n",
    "result = newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert redis entries to dataframe\n",
    "df = pd.DataFrame(columns = ['Tweets', 'User', 'User_statuses_count', \n",
    "                             'user_followers', 'User_location', 'User_verified',\n",
    "                             'fav_count', 'rt_count', 'tweet_date'])\n",
    "\n",
    "index = 0\n",
    "for x in result:\n",
    "    d = json.loads(x)\n",
    "    df.loc[index, 'Tweets'] = d.get('text')\n",
    "    df.loc[index, 'User'] = d.get('user')\n",
    "    df.loc[index, 'User_statuses_count'] = d.get('user_statuses_count')\n",
    "    df.loc[index, 'user_followers'] = d.get('user_followers')\n",
    "    df.loc[index, 'user_friends'] = d.get('user_friends')\n",
    "    df.loc[index, 'listed_count'] = d.get('listed_count')\n",
    "    df.loc[index, 'User_location'] = d.get('user_location')\n",
    "    df.loc[index, 'User_verified'] = d.get('user_verified')\n",
    "    df.loc[index, 'fav_count'] = d.get('fav_count')\n",
    "    df.loc[index, 'rt_count'] = d.get('rt_count')\n",
    "    df.loc[index, 'tweet_date'] = d.get('tweet_date')\n",
    "    index += 1\n",
    "    \n",
    "df['fav_count'].replace('None', 0, inplace=True)    \n",
    "df['rt_count'].replace('None', 0, inplace=True) \n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean greek tweets - Greek Stemmer (https://github.com/DimitrisCC/GrPolitics_Twitter_SentAnalysis/blob/5c165306f3cb00d001942013fd252589614a13f9/preprocessing.py)\n",
    "\n",
    "class GreekAnalyzer:\n",
    "    one_suff = ('Α', 'Ο', 'Ε', 'Η', 'Ω', 'Υ', 'Ι')\n",
    "    three_suff = ('ΟΥΣ', 'ΕΙΣ', 'ΕΩΝ', 'ΟΥΝ')\n",
    "    two_suff = ('ΟΣ', 'ΗΣ', 'ΕΣ', 'ΩΝ', 'ΟΥ', 'ΟΙ', 'ΑΣ', 'ΩΣ', 'ΑΙ', 'ΥΣ', 'ΟΝ', 'ΑΝ', 'ΕΙ')\n",
    "\n",
    "    class Sentence:\n",
    "        # This class represents a string which will be cleaned as part of a pre-processing procedure\n",
    "        def __init__(self, sentence):\n",
    "            self.sentence = str(sentence).upper()\n",
    "\n",
    "        def __repr__(self):\n",
    "            return str(self.sentence)\n",
    "\n",
    "        # Default argument values are evaluated at function define-time,\n",
    "        # but self is an argument only available at function call time.\n",
    "        # Thus arguments in the argument list cannot refer each other.\n",
    "\n",
    "        def strip_accents(self, sentence=None):\n",
    "            if sentence is None:\n",
    "                sentence = self.sentence\n",
    "            return GreekAnalyzer.Sentence(''.join(c for c in unicodedata.normalize('NFD', sentence)\n",
    "                                                  if unicodedata.category(c) != 'Mn'))\n",
    "\n",
    "        def strip_specialcharacters_numbers(self, sentence=None):\n",
    "            if sentence is None:\n",
    "                sentence = self.sentence\n",
    "            return GreekAnalyzer.Sentence(re.sub(r'[^Α-Ωα-ω ]', '', sentence, flags=re.MULTILINE))\n",
    "\n",
    "        def strip_links(self, sentence=None):\n",
    "            if sentence is None:\n",
    "                sentence = self.sentence\n",
    "            return GreekAnalyzer.Sentence(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE))\n",
    "\n",
    "        def strip_tags(self, sentence=None):\n",
    "            if sentence is None:\n",
    "                sentence = self.sentence\n",
    "            return GreekAnalyzer.Sentence(re.sub(r'#\\w*|@\\w*', '', sentence, flags=re.MULTILINE))\n",
    "\n",
    "        def stem(self, sentence=None):\n",
    "            if sentence is None:\n",
    "                sentence = self.sentence\n",
    "            stemmed = \"\"\n",
    "            for term in sentence.split():\n",
    "                # Check if term is numeric\n",
    "                pattern = re.compile(\"^[+-]?(\\\\d+(\\\\.\\\\d*)?|\\\\.\\\\d+)([eE][+-]?\\\\d+)?$\")\n",
    "                if pattern.match(term):\n",
    "                    return ''\n",
    "                # Remove first level suffixes only if the term is 4 letters or more\n",
    "                if len(term) >= 4:\n",
    "                    # Remove the 3 letter suffixes\n",
    "                    if term.endswith(GreekAnalyzer.three_suff):\n",
    "                        term = term[:-3]\n",
    "                        # Remove the 2 letter suffixes\n",
    "                    elif term.endswith(GreekAnalyzer.two_suff):\n",
    "                        term = term[:-2]\n",
    "                    # Remove the 1 letter suffixes\n",
    "                    elif term.endswith(GreekAnalyzer.one_suff):\n",
    "                        term = term[:-1]\n",
    "                stemmed += term + ' '\n",
    "            return GreekAnalyzer.Sentence(stemmed[:-1])\n",
    "\n",
    "        def strip_stopwords(self, sentence=None, stop_words=None):\n",
    "            if sentence is None:\n",
    "                sentence = self.sentence\n",
    "            if stop_words is None:\n",
    "                return GreekAnalyzer.Sentence(sentence)\n",
    "            for w in stop_words:\n",
    "                sentence = re.sub(r'\\b'+w+r'\\b', '', sentence)\n",
    "            return GreekAnalyzer.Sentence(sentence)\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        if isinstance(sentence, GreekAnalyzer.Sentence):\n",
    "            self.sentence = sentence\n",
    "        else:\n",
    "            self.sentence = GreekAnalyzer.Sentence(sentence)\n",
    "\n",
    "    def clean(self, sentence=None, stop_words=None):\n",
    "        if sentence is None:\n",
    "            sentence = self.sentence\n",
    "        if isinstance(sentence, GreekAnalyzer.Sentence):\n",
    "            return str(sentence\n",
    "                       .strip_accents()\n",
    "                       .strip_links()\n",
    "                       .strip_tags()\n",
    "                       .strip_specialcharacters_numbers()\n",
    "                       .strip_stopwords(stop_words=stop_words).stem()\n",
    "                       )\n",
    "        else:\n",
    "            return GreekAnalyzer(GreekAnalyzer.Sentence(sentence)).clean(stop_words)\n",
    "\n",
    "\n",
    "#Loading stopwords\n",
    "fstopwords = open('greekstopwords.txt', 'rt', encoding=\"utf8\")\n",
    "\n",
    "stopwords = [w.strip() for w in fstopwords.readlines() if w.strip() != '']\n",
    "del (stopwords[0])  \n",
    "\n",
    "fstopwords.close()\n",
    "\n",
    "\n",
    "def clean_tweets(tweets: dict):\n",
    "    proc = []\n",
    "    for text in tweets['text']:\n",
    "        analyzer = GreekAnalyzer(text)\n",
    "        proc.append(analyzer.clean(stop_words=stopwords))\n",
    "    tweets['clean_text'] = proc\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def format_time(time):\n",
    "    strtime = str(time)\n",
    "    digits = len(strtime)\n",
    "    if digits == 1:\n",
    "        return \"0\"+strtime\n",
    "    else:\n",
    "        return strtime\n",
    "    \n",
    "for i in range(len(df)) :\n",
    "    analyzer = GreekAnalyzer(df.iloc[i,0])\n",
    "    df.iloc[i,0] = analyzer.clean(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 Fake account detection\n",
    "\n",
    "train_data = pd.read_csv('kaggle_train.csv')\n",
    "test_data = df\n",
    "\n",
    "train_attr = train_data[\n",
    "  ['followers_count', 'friends_count', 'listedcount', 'favourites_count', 'statuses_count', 'verified']]\n",
    "train_label = train_data[['bot']]\n",
    "\n",
    "test_attr = test_data[\n",
    "  ['user_followers', 'user_friends', 'listed_count', 'fav_count', 'User_statuses_count', 'User_verified']]\n",
    "test_attr.columns = ['followers_count','friends_count','listedcount','favourites_count','statuses_count','verified']\n",
    "\n",
    "train_attr = normalize(train_attr)\n",
    "test_attr = normalize(test_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB().fit(train_attr, train_label.as_matrix())\n",
    "predicted = nb.predict(test_attr)\n",
    "pred = np.array(predicted)\n",
    "\n",
    "test_data[\"pred\"] = pred\n",
    "\n",
    "count1 = test_data[test_data[\"pred\"]==1].count()[\"pred\"]\n",
    "print(\"Real Accounts : \",count1) \n",
    "count0 = test_data[test_data[\"pred\"]==0].count()[\"pred\"]\n",
    "print(\"Fake accounts : \",count0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only the real users\n",
    "test_data = test_data[test_data[\"pred\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "tweets = test_data[\"Tweets\"]\n",
    "\n",
    "#send tweets to Spark\n",
    "for i in tweets:\n",
    "    print(i)\n",
    "    producer.send(\"<your_kafka_topic_name>\", i)\n",
    "    producer.flush()\n",
    "\n",
    "print(\"-------------All tweets are headed to spark streaming------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
